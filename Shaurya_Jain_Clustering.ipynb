import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns  # Added for visualization


customers = pd.read_csv("Customers.csv")
transactions = pd.read_csv("Transactions.csv")


rfm = transactions.groupby("CustomerID").agg({
    "TransactionDate": lambda x: (pd.to_datetime("today") - pd.to_datetime(x.max())).days,  # Removed .date()
    "TransactionID": "count",
    "TotalValue": "sum"
}).rename(columns={
    "TransactionDate": "Recency",
    "TransactionID": "Frequency",
    "TotalValue": "Monetary"
})

rfm = pd.merge(rfm, customers, on="CustomerID")
rfm["SignupVintage"] = (pd.to_datetime("today") - pd.to_datetime(rfm["SignupDate"])).dt.days
rfm = pd.get_dummies(rfm, columns=["Region"])


scaler = StandardScaler()
scaled_data = scaler.fit_transform(rfm.drop(["CustomerID", "CustomerName", "SignupDate"], axis=1))


pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(scaled_data)


db_index = davies_bouldin_score(scaled_data, clusters)
print(f"DB Index: {db_index:.2f}")  # Example Output: DB Index: 1.25


plt.figure(figsize=(10, 6))
sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=clusters, palette="tab10")
plt.title("Customer Clusters (PCA-Reduced)")
plt.savefig("clusters.png")
